import org.apache.spark.sql.SparkSession
import org.apache.hadoop.hbase.{HBaseConfiguration, TableName}
import org.apache.hadoop.hbase.client.{ConnectionFactory, Scan, Put}
import org.apache.hadoop.hbase.util.Bytes

object HBaseCopyLimitedRows {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .appName("HBaseCopyLimitedRows")
      .getOrCreate()

    val hbaseConf = HBaseConfiguration.create()
    val tableNameA = "A"
    val tableNameB = "B"
    val limit = 1000 // Number of rows to copy

    val scan = new Scan()

    val connection = ConnectionFactory.createConnection(hbaseConf)
    val tableA = connection.getTable(TableName.valueOf(tableNameA))
    val resultScanner = tableA.getScanner(scan)

    val limitedRows = resultScanner.iterator().asScala.take(limit).toSeq

    val tableB = connection.getTable(TableName.valueOf(tableNameB))
    limitedRows.foreach { result =>
      val rowKey = Bytes.toString(result.getRow)
      val clientRequestId = Bytes.toString(result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("clientrequestid")))
      val ingestionTime = Bytes.toString(result.getValue(Bytes.toBytes("cf"), Bytes.toBytes("ingestion_time")))

      val put = new Put(Bytes.toBytes(rowKey))
      put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("clientrequestid"), Bytes.toBytes(clientRequestId))
      put.addColumn(Bytes.toBytes("cf"), Bytes.toBytes("ingestion_time"), Bytes.toBytes(ingestionTime))
      tableB.put(put)
    }

    resultScanner.close()
    tableA.close()
    tableB.close()
    connection.close()

    spark.stop()
  }
}
